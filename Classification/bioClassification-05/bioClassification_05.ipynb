{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Proje Yöntemi**\n",
        "\n",
        "Bu projede, DNA dizilimleri gibi sıralı (sekans) yapıya sahip verilerin modellenmesinde etkin sonuçlar üretmesi nedeniyle Uzun Kısa Süreli Bellek (Long Short-Term Memory, LSTM) tabanlı bir derin öğrenme mimarisi kullanılmıştır. LSTM modeli, dizisel bağımlılıkları öğrenme yeteneği sayesinde biyolojik dizilerin analizinde tercih edilmiştir.\n",
        "\n",
        "Kullanılan veri seti sınıf dağılımı açısından dengeli olmasına rağmen, modelin genelleme kabiliyetini artırmak amacıyla veri çoğaltma (data augmentation) yöntemi uygulanmıştır. Ham veri setinde etiket bilgisi eksik olan örnekler tespit edilerek analiz dışı bırakılmıştır. Ardından, her bir DNA dizisi için etiketler tanımlanmış ve nükleotitler (A, T, C, G) sayısal temsillere dönüştürülerek modele uygun hale getirilmiştir.\n",
        "\n",
        "Değişken uzunluktaki dizilerin modele tutarlı bir biçimde aktarılabilmesi için tüm dizilere sabit uzunluk kazandırmak amacıyla padding işlemi uygulanmıştır. Eğitim sürecinde, DNA dizilerinin daha anlamlı ve düşük boyutlu vektör temsillerine dönüştürülmesi amacıyla Embedding katmanı kullanılmış; bu yaklaşım aynı zamanda modelin öğrenme sürecini hızlandırmıştır.\n",
        "\n",
        "Hesaplama maliyetini ve bellek kullanımını azaltmak amacıyla, veri seti modele parça parça aktarılan *chunking* yöntemiyle işlenmiştir. Önceden uygulanan padding işlemi nedeniyle oluşan boş veri parçalarının (empty chunks) modele dahil edilmemesi için ek bir kontrol mekanizması geliştirilmiştir.\n",
        "\n",
        "Modelin performansını değerlendirmek için kayıp fonksiyonu olarak *binary cross-entropy*, optimizasyon algoritması olarak ise RMSprop tercih edilmiştir. Kod yapısı, her bir işlem adımının ayrı fonksiyonlar halinde tanımlanmasıyla oluşturulmuş; bu sayede kodun okunabilirliği artırılmış ve ilerleyen aşamalarda yapılacak düzenlemeler kolaylaştırılmıştır.\n",
        "\n",
        "Model geliştirme sürecinde, başlangıçta temel bir mimari oluşturulmuş ve elde edilen sonuçlara bağlı olarak epoch sayısı, öğrenme oranı (*learning rate*), batch size ve maksimum dizi uzunluğu (*max sequence length*) gibi hiperparametreler üzerinde çeşitli denemeler yapılmıştır. Elde edilen sonuçların beklenen seviyeye ulaşamamasının temel nedeninin veri setinin büyüklüğü olduğu düşünülmektedir. Her ne kadar veri seti dengeli bir yapıya sahip olsa da, yalnızca 80 protein dizilimi içermektedir. Daha büyük ölçekli, yaklaşık bin protein dizilimini kapsayan bir veri seti ile modelin performansının anlamlı ölçüde artacağı öngörülmektedir.\n",
        "\n",
        "---\n",
        "\n",
        "### **Veri Seti Oluşturma Süreci (Veri Seti Hikâyesi)**\n",
        "\n",
        "Bu çalışmada kullanılan veri seti, 40 adet fonksiyonel ve 40 adet fonksiyonel olmayan olmak üzere toplam 80 DNA diziliminden oluşmaktadır. Her bir dizilim yaklaşık 30.000 ila 50.000 nükleotit uzunluğuna sahiptir. Veriler, farklı biyoinformatik kaynaklardan ve toplam 80 ayrı siteden elde edilerek derlenmiştir. Bu süreç sonucunda, sınıflar arasında dengeli bir dağılıma sahip özgün bir veri seti oluşturulmuştur.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "9leWxRnaJ04N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7WjPGNFGxzOy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36b2e500-68d9-4244-86aa-6382579c7d1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Veri başarıyla yüklendi!\n",
            "Satır Sayısı: 45633, Sütun Sayısı: 2\n",
            "\n",
            "Veri Türleri:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 45633 entries, 0 to 45632\n",
            "Data columns (total 2 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Sequence  45633 non-null  object \n",
            " 1   Label     80 non-null     float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 713.1+ KB\n",
            "None\n",
            "\n",
            "Etiket Dağılımı:\n",
            "Label\n",
            "0.0    40\n",
            "1.0    40\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Eksik Veri Sayısı:\n",
            "Sequence        0\n",
            "Label       45553\n",
            "dtype: int64\n",
            "\n",
            "Veri dengelendi. Yeni etiket dağılımı:\n",
            "Label\n",
            "0.0    40\n",
            "1.0    40\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Model eğitiliyor...\n",
            "Toplam 1 chunk işlenecek.\n",
            "\n",
            "Epoch 1/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 3s/step - accuracy: 0.6593 - loss: 0.6912\n",
            "\n",
            "Epoch 2/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.4344 - loss: 0.6954\n",
            "\n",
            "Epoch 3/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3s/step - accuracy: 0.5443 - loss: 0.6918\n",
            "\n",
            "Epoch 4/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.4706 - loss: 0.6933\n",
            "\n",
            "Epoch 5/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.4697 - loss: 0.6960\n",
            "\n",
            "Epoch 6/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.5634 - loss: 0.6883\n",
            "\n",
            "Epoch 7/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 5s/step - accuracy: 0.4541 - loss: 0.6945\n",
            "\n",
            "Epoch 8/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4s/step - accuracy: 0.5814 - loss: 0.6918\n",
            "\n",
            "Epoch 9/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.5009 - loss: 0.6891\n",
            "\n",
            "Epoch 10/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.5629 - loss: 0.6903\n",
            "\n",
            "Epoch 11/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.4255 - loss: 0.6967\n",
            "\n",
            "Epoch 12/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.4160 - loss: 0.6950\n",
            "\n",
            "Epoch 13/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.4813 - loss: 0.6934\n",
            "\n",
            "Epoch 14/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.5524 - loss: 0.6898\n",
            "\n",
            "Epoch 15/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.5363 - loss: 0.6906\n",
            "\n",
            "Epoch 16/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.6448 - loss: 0.6899\n",
            "\n",
            "Epoch 17/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.4791 - loss: 0.6922\n",
            "\n",
            "Epoch 18/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.5029 - loss: 0.6925\n",
            "\n",
            "Epoch 19/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.6007 - loss: 0.6912\n",
            "\n",
            "Epoch 20/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.4778 - loss: 0.6950\n",
            "\n",
            "Epoch 21/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.6093 - loss: 0.6893\n",
            "\n",
            "Epoch 22/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3s/step - accuracy: 0.6481 - loss: 0.6873\n",
            "\n",
            "Epoch 23/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3s/step - accuracy: 0.5329 - loss: 0.6919\n",
            "\n",
            "Epoch 24/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 3s/step - accuracy: 0.5247 - loss: 0.6940\n",
            "\n",
            "Epoch 25/25\n",
            "Chunk boyutu: (64, 1800), y_chunk boyutu: (64,)\n",
            "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 3s/step - accuracy: 0.5604 - loss: 0.6932\n",
            "\n",
            "Model değerlendiriliyor...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 849ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0       0.00      0.00      0.00        11\n",
            "         1.0       0.31      1.00      0.48         5\n",
            "\n",
            "    accuracy                           0.31        16\n",
            "   macro avg       0.16      0.50      0.24        16\n",
            "weighted avg       0.10      0.31      0.15        16\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 992ms/step - accuracy: 0.3125 - loss: 0.6973\n",
            "\n",
            "Test Doğruluğu: 0.31\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils import resample\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Veri yükleme\n",
        "def load_data(file_path):\n",
        "    # Belirtilen dosya yolundan veriyi yükler ve temel bilgiler verir.\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(\"Veri başarıyla yüklendi!\")\n",
        "    print(f\"Satır Sayısı: {data.shape[0]}, Sütun Sayısı: {data.shape[1]}\")\n",
        "    return data\n",
        "\n",
        "# Veri keşfi\n",
        "def explore_data(data):\n",
        "    # Veri seti hakkında bilgi ve istatistikleri ekrana yazdırır.\n",
        "    print(\"\\nVeri Türleri:\")\n",
        "    print(data.info())\n",
        "    print(\"\\nEtiket Dağılımı:\")\n",
        "    print(data['Label'].value_counts())\n",
        "    print(\"\\nEksik Veri Sayısı:\")\n",
        "    print(data.isnull().sum())\n",
        "\n",
        "# Veri artırma (class balancing)\n",
        "def balance_data(data):\n",
        "    # Veri setindeki sınıfların dengesini sağlar.\n",
        "    class_0 = data[data['Label'] == 0]\n",
        "    class_1 = data[data['Label'] == 1]\n",
        "\n",
        "    # Az olan sınıfı artırarak dengeler.\n",
        "    class_1_upsampled = resample(class_1,\n",
        "                                 replace=True,\n",
        "                                 n_samples=len(class_0),\n",
        "                                 random_state=42)\n",
        "\n",
        "    # Dengeli veri setini birleştirir.\n",
        "    balanced_data = pd.concat([class_0, class_1_upsampled])\n",
        "    print(\"\\nVeri dengelendi. Yeni etiket dağılımı:\")\n",
        "    print(balanced_data['Label'].value_counts())\n",
        "    return balanced_data.reset_index(drop=True)\n",
        "\n",
        "# Veri ön işleme\n",
        "def preprocess_data(data, max_sequence_length=1800):\n",
        "    # Eksik etiketleri temizler ve DNA dizilerini uygun formatta işler.\n",
        "\n",
        "    # Etiketlerde eksik olan satırları çıkarır.\n",
        "    data = data.dropna(subset=['Label']).reset_index(drop=True)\n",
        "\n",
        "    # Etiketleri sayısal değerlere kodlar.\n",
        "    label_encoder = LabelEncoder()\n",
        "    data.loc[:, 'Label'] = label_encoder.fit_transform(data['Label'])  # 0 ve 1 kodlaması\n",
        "\n",
        "    # DNA dizilerini sayısal formatta kodlar.\n",
        "    nucleotide_map = {'A': 1, 'T': 2, 'G': 3, 'C': 4}  # 0 padding için ayrıldı\n",
        "    sequences_encoded = data['Sequence'].apply(\n",
        "        lambda seq: [nucleotide_map[char] for char in seq]\n",
        "    )\n",
        "\n",
        "    # Dizileri aynı uzunluğa getirir (padding).\n",
        "    if not max_sequence_length:\n",
        "        max_sequence_length = max(sequences_encoded.apply(len))\n",
        "    sequences_padded = np.array([\n",
        "        seq + [0] * (max_sequence_length - len(seq)) if len(seq) < max_sequence_length else seq[:max_sequence_length]\n",
        "        for seq in sequences_encoded\n",
        "    ])\n",
        "\n",
        "    return sequences_padded, data['Label'], max_sequence_length\n",
        "\n",
        "# LSTM modeli oluşturma\n",
        "def create_lstm_model(max_sequence_length):\n",
        "    # LSTM tabanlı model oluşturur.\n",
        "    model = Sequential([\n",
        "        Embedding(input_dim=5, output_dim=64, input_length=max_sequence_length),\n",
        "        LSTM(128, return_sequences=True),\n",
        "        Dropout(0.5),\n",
        "        LSTM(64),\n",
        "        Dropout(0.5),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Modeli RMSprop optimizer ile derler.\n",
        "    model.compile(optimizer=RMSprop(learning_rate=0.00001),\n",
        "                  loss='binary_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Chunking ile veri işleme ve eğitim\n",
        "def train_model_with_chunking(model, data, chunk_size, max_sequence_length, epochs, batch_size):\n",
        "    # Veriyi parça parça işler ve modeli eğitir.Belleği yormamak için.\n",
        "    num_chunks = len(data) // chunk_size + int(len(data) % chunk_size != 0)\n",
        "    print(f\"Toplam {num_chunks} chunk işlenecek.\")\n",
        "\n",
        "    #modeli chunkin ile eğitilmesi için döngü oluşturur\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch + 1}/{epochs}\")\n",
        "        for i in range(0, len(data), chunk_size):\n",
        "            chunk = data.iloc[i:i + chunk_size]\n",
        "            X_chunk, y_chunk, _ = preprocess_data(chunk, max_sequence_length)\n",
        "            y_chunk = np.array(y_chunk)\n",
        "\n",
        "            # Chunk'ın dolu olup olmadığını kontrol eder\n",
        "            if X_chunk.size == 0 or y_chunk.size == 0:\n",
        "                print(\"Boş chunk atlandı.\")\n",
        "                continue\n",
        "\n",
        "            # LSTM modeli için X_chunk'ın şekli\n",
        "            X_chunk = np.array(X_chunk).reshape(-1, max_sequence_length)\n",
        "\n",
        "            print(f\"Chunk boyutu: {X_chunk.shape}, y_chunk boyutu: {y_chunk.shape}\")\n",
        "\n",
        "            # Modeli eğitir\n",
        "            model.fit(X_chunk, y_chunk, batch_size=batch_size, epochs=1, verbose=1)\n",
        "    return model\n",
        "\n",
        "# Ana iş akışı\n",
        "def main():\n",
        "    file_path = 'data.txt'\n",
        "\n",
        "    # 1. Veri Yükleme\n",
        "    data = load_data(file_path)\n",
        "\n",
        "    # 2. Veri Keşfi\n",
        "    explore_data(data)\n",
        "\n",
        "    # 3. Veri Dengesi\n",
        "    data = balance_data(data)\n",
        "\n",
        "    # 4. Veri Ön İşleme\n",
        "    X, y, max_sequence_length = preprocess_data(data)\n",
        "\n",
        "    # 5. Eğitim ve test setini bölme\n",
        "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 6. Model Oluşturma\n",
        "    model = create_lstm_model(max_sequence_length)\n",
        "\n",
        "    # 7. Chunking ile Model Eğitimi\n",
        "    print(\"\\nModel eğitiliyor...\")\n",
        "    trained_model = train_model_with_chunking(\n",
        "        model, train_data, chunk_size=900, max_sequence_length=max_sequence_length, epochs=25, batch_size=11\n",
        "    )\n",
        "\n",
        "    # 8. Model Değerlendirme\n",
        "    #test_data modelin anlayabileceği formata dönüştürülür.\n",
        "    print(\"\\nModel değerlendiriliyor...\")\n",
        "    X_test, y_test, _ = preprocess_data(test_data, max_sequence_length)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    #Olasılık değeri 0.5'ten büyükse, model bu DNA dizisinin \"fonksiyonel\" olduğunu tahmin eder.\n",
        "    #Aksi halde \"fonksiyonel değil\" (0) tahmini yapılır. astype tahmin edilen değerleri tam sayıya (0 veya 1) dönüştürür.\n",
        "    predictions = (trained_model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "\n",
        "    # test veri seti için gerçek etiketlerle modelin tahminlerini karşılaştırır\n",
        "    print(classification_report(y_test, predictions))\n",
        "    loss, accuracy = trained_model.evaluate(X_test, y_test, verbose=1)\n",
        "    print(f\"\\nTest Doğruluğu: {accuracy:.2f}\")\n",
        "\n",
        "# Çalıştırma\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''VERİ SETİ BU SİTELERDEN OLUŞTURULMUŞTUR\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_033787.2?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_031916.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_012177.2?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_015845.2?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_028088.2?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_046865.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_029095.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_009876.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_023279.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_008116.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_009215.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_012805.2?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JALYAP010000018.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAOCZR010000006.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAOCZR010000013.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JALYCG010000020.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAPKJG010000051.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JARWTQ010000032.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAOCZR010000023.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAOCZR010000034.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAOCZR010000039.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JABUOZ010000054.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/PP520311.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/PP521662.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_041796.3?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAVKLT010000050.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAVKLT010000058.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAVKLT010000019.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAVKLT010000027.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAVKLT010000042.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAVKLT010000060.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAVKLT010000063.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAVKLT010000069.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAVKLT010000075.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JARXWJ010000024.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_031823.2?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_VUMI01000042.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_VUMI01000048.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_VUMI01000057.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_VUMI01000064.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JACBYI010000053.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAXCVI010000005.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NG_031992.2?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAMWGN010000012.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/CAYFAD010000064.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAMWGN010000016.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NW_019212895.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/D64044.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/AF503504.2?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/OQ626729.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/KZ804422.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/AJYU02000107.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NMSE01000019.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/MLCZ01000020.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/QXFV01000144.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/CAXKWB010105999.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAMWGO010000013.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JANXRJ010000030.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/NZ_JAXCVJ010000010.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/LTZY01000037.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/KV793378.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/MCXF01000102.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/MCYC01000036.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/MCZD01000178.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/MCZL01000104.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/MDAW01000153.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/MDBF01000109.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/POWH01000010.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/PDNQ01002977.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/PJQY01003117.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/PIFU01000042.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/AUSU01000465.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/AYVE01000029.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/CBTQ010000016.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/CH478980.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/JYRW01000044.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/JYUI01000046.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/MIIY01000044.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/LXGZ01000144.1?report=fasta\n",
        "https://www.ncbi.nlm.nih.gov/nuccore/LWMZ01000098.1?report=fasta'''"
      ],
      "metadata": {
        "id": "__ga8CfFzQsa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}